= 第二节 监督学习与梯度下降 =

== 梯度下降 ==
1. 批量梯度下降
    每一次θ更新，使用所有的训练样本
2. 随机（增量）梯度下降
    每一次θ更新，使用一个训练样本
    调参速度快很多, 但不会精确收敛到全局的最小值
    
 
= 第三节 欠拟合与过拟合=

欠拟合 underfitting
过拟合 overfitting

== 参数学习算法 ==
有固定数目的参数，以用来进行数据拟合的算法
eg. 线性拟合

== 非参数学习算法 ==
参数会随着m增长的算法, m:训练集合的大小

=== 局部加权回归 Local Regression (Loess) ===
对某一个点进行局部加权回归时，该点附近的邻域权值高，远离该点的权值低
权值通常使用指数衰减函数

对比线性回归：
对于某一个确定的x，想求出x处的假设函数h
1. 线性回归：先拟合出θ，使得代价函数J最小化, 返回拟合后的线性函数
2. 局部加权回归：处理x点时，检查数据集合，只考虑那些位于x周围固定区域内的数据点，然后对这个数据子集使用线性回归，拟合出一条直线，然后根据这条直线求出具体的值，作为算法的返回结果; 对每一个点都需要重新进行一遍全新的拟合过程(应用：直升机自动驾驶) (扩展KD-tree，优化此算法性能)

采用频率学派的观点, 将θ视为未知的常量，而非随机变量
似然性L(θ): 强调将概率视为θ的函数
术语：数据的概率和参数的似然性

IID: Independently and Identically Idstributed 独立同分布

极大似然估计：选择θ使似然性最大

== 分类算法 ==
g(z) sigmoid函数(logestic函数)

=== 感知器算法 ===
g(z) 0 1

== 牛顿迭代法 ==
f(θ) = 0
比梯度下降法收敛速度快
理论上是二次收敛：每次迭代后，误差会变为之前的平方

牛顿迭代法与梯度下降（上升）法比较
1. 优点：牛顿迭代法收敛所需要的迭代次数要少得多
2. 缺点：每一次迭代，都要重新计算一次Hession矩阵(n*n的矩阵, n:特征的数量)的逆

== 广义线性模型 ==
=== 指数分布族 ===
伯努利分布和高斯分布式指数分布族的特例
